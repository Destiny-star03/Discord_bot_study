# crawlers/school_notice_detail.py
from urllib.parse import urljoin, urlsplit
import re
import requests
import base64
from bs4 import BeautifulSoup

# javascript:fn_egov_downFile('ATCH_FILE_ID','1');
DOWN_RE = re.compile(
    r"fn_egov_downFile\(\s*'([^']+)'\s*,\s*'([^']+)'\s*\)", re.IGNORECASE
)

# data:image/png;base64,....
DATA_URL_RE = re.compile(r"^data:(image/[a-zA-Z0-9.+-]+);base64,(.+)$", re.DOTALL)


def _is_noisy_text(text: str) -> bool:
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    if len(lines) >= 40:
        short = sum(1 for ln in lines if len(ln) <= 2)
        if short / len(lines) >= 0.35:
            return True
    if text.count("\n") >= 120:
        return True
    return False


def _normalize_broken_text(text: str) -> str:
    # 0) ê³µë°±ì„ í•œ ì¤„ë¡œ ì •ë¦¬
    t = re.sub(r"\s+", " ", text).strip()

    # 1) ë‚ ì§œ ë¶™ì´ê¸°
    # 1-1) 4ìë¦¬ ì—°ë„: 2025. 11. 26. -> 2025.11.26.
    t = re.sub(r"(\d{4})\s*\.\s*(\d{1,2})\s*\.\s*(\d{1,2})\s*\.", r"\1.\2.\3.", t)
    # 1-2) 2ìë¦¬ ì—°ë„: 25. 11. 26. -> 25.11.26.
    t = re.sub(
        r"(?<!\d)(\d{2})\s*\.\s*(\d{1,2})\s*\.\s*(\d{1,2})\s*\.", r"\1.\2.\3.", t
    )

    # 2) ì‹œê°„ ë‹¨ìœ„ ë¶™ì´ê¸°: 15 ì‹œ -> 15ì‹œ, 14 ì‹œ 30 ë¶„ -> 14ì‹œ 30ë¶„
    t = re.sub(r"(\d)\s+(ì‹œ|ë¶„|ì´ˆ|í˜¸|ì¼|ì›”|ë…„)", r"\1\2", t)

    # 3) ì „í™”ë²ˆí˜¸ ë¶™ì´ê¸°: (055 751 2088) -> (055-751-2088)
    t = re.sub(r"\((\d{2,4})\s+(\d{3,4})\s+(\d{4})\)", r"(\1-\2-\3)", t)

    # 4) ê¸°í˜¸ ì£¼ë³€ ê³µë°± ì •ë¦¬
    t = re.sub(r"\s*~\s*", "~", t)
    t = re.sub(r"\s+([:;,.!?])", r"\1", t)
    t = re.sub(r"\(\s+", "(", t)
    t = re.sub(r"\s+\)", ")", t)

    # 5) ì„¹ì…˜/êµ¬ë¶„ì ì¤„ë°”ê¿ˆ
    t = re.sub(r"\s*â– \s*", "\nâ–  ", t)

    # 6) â˜…â˜…â˜… í—¤ë” ê°€ë…ì„± ê°•í™”(ì¤„ ë„ìš°ê³  êµµê²Œ)
    t = re.sub(r"\s*(â˜…{3}\s*[^â˜…]+?\s*â˜…{3})\s*", r"\n\n**\1**\n", t)

    # 7) ë™ê·¸ë¼ë¯¸ ë²ˆí˜¸ ì¤„ë°”ê¿ˆ
    t = re.sub(r"\s*([â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©])\s*", r"\n\1 ", t)

    # 8) '1. 2. 3.' ëª©ë¡ ì¤„ë°”ê¿ˆ
    #    âœ… (?!\d{1,2}\.) ì¶”ê°€: "25.11." ê°™ì€ ë‚ ì§œëŠ” ëª©ë¡ìœ¼ë¡œ ë³´ì§€ ì•ŠìŒ
    t = re.sub(
        r"(?<!\d\.)(?<![0-9A-Za-zê°€-í£])([1-9]|[1-9]\d)\.\s*(?!\d{1,2}\.)", r"\n\1. ", t
    )

    # 9) í•˜ì´í”ˆ í•­ëª©/ì£¼ì˜ë¬¸ ì¤„ë°”ê¿ˆ
    t = re.sub(r"\s+-\s+", "\n- ", t)
    t = re.sub(r"\s*â€»\s*", "\nâ€» ", t)

    # 10) â€œë¬¸ì˜ì²˜:â€ëŠ” ë³„ë„ ì¤„ë¡œ
    t = re.sub(r"\s*(ë¬¸ì˜ì²˜\s*:)", r"\n\1", t)

    # 11) ì¤„ë°”ê¿ˆ ì •ë¦¬
    t = re.sub(r"\n{3,}", "\n\n", t).strip()

    return t


def _clean_text(text: str) -> str:
    lines = [line.strip() for line in text.splitlines()]
    lines = [line for line in lines if line]
    return "\n".join(lines)


def _extract_body_text(wrap) -> str:
    raw_nl = wrap.get_text("\n", strip=True)
    if not _is_noisy_text(raw_nl):
        return _clean_text(raw_nl)

    raw_sp = wrap.get_text(" ", strip=True)
    return _normalize_broken_text(raw_sp)


def _build_download_url(detail_url: str, atch_file_id: str, file_sn: str) -> str:
    parts = urlsplit(detail_url)
    base = f"{parts.scheme}://{parts.netloc}"
    return f"{base}/cmm/fms/FileDown.do?atchFileId={atch_file_id}&fileSn={file_sn}"


def decode_data_image(data_url: str):
    """
    return: (mime, ext, bytes)
    """
    m = DATA_URL_RE.match(data_url.strip())
    if not m:
        raise ValueError("Invalid data:image base64 URL")

    mime = m.group(1)  # e.g. image/png
    b64 = m.group(2)

    # base64 ë¬¸ìì—´ ë‚´ ê³µë°±/ê°œí–‰ ì œê±°
    b64 = re.sub(r"\s+", "", b64)

    raw = base64.b64decode(b64)

    ext = mime.split("/")[-1].lower()
    if ext == "jpeg":
        ext = "jpg"

    return mime, ext, raw


def _cell_text(cell) -> str:
    t = cell.get_text(" ", strip=True)
    # ì˜ˆ: "16:10~16~:25" ê°™ì€ ì˜¤íƒ€/ê¹¨ì§ ì •ë¦¬(í•„ìš”ì‹œ í™•ì¥)
    t = re.sub(r"(\d{1,2}):(\d{2})\s*~\s*(\d{1,2})\s*~\s*:(\d{2})", r"\1:\2~\3:\4", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t


def _table_to_grid(table) -> list[list[str]]:
    grid = []
    span_map = {}  # col_idx -> [text, remaining_rows]

    for tr in table.find_all("tr"):
        row = []
        col = 0

        def fill_spans_until():
            nonlocal col, row
            while col in span_map:
                txt, remain = span_map[col]
                row.append(txt)
                remain -= 1
                if remain <= 0:
                    del span_map[col]
                else:
                    span_map[col] = [txt, remain]
                col += 1

        fill_spans_until()

        cells = tr.find_all(["th", "td"])
        for cell in cells:
            fill_spans_until()

            txt = _cell_text(cell)
            rs = int(cell.get("rowspan", 1) or 1)
            cs = int(cell.get("colspan", 1) or 1)

            for i in range(cs):
                row.append(txt if i == 0 else "")
                if rs > 1:
                    span_map[col + i] = [txt, rs - 1]
            col += cs

        # í–‰ ëì—ì„œë„ ë‚¨ì€ spanì´ ìˆìœ¼ë©´ ì±„ì›Œì„œ ì—´ ìˆ˜ ë§ì¶”ê¸°
        fill_spans_until()

        grid.append(row)

    # í–‰ë³„ ì—´ ê°œìˆ˜ ë§ì¶”ê¸°
    max_cols = max((len(r) for r in grid), default=0)
    for r in grid:
        if len(r) < max_cols:
            r.extend([""] * (max_cols - len(r)))
    return grid


def _grid_to_codeblock(grid: list[list[str]], max_width: int = 80) -> str:
    if not grid:
        return ""

    # ë„ˆë¬´ ê¸´ ì…€ì€ ìë¥´ê¸°
    def clip(s: str, n: int = 24) -> str:
        s = s or ""
        return s if len(s) <= n else (s[: n - 1] + "â€¦")

    clipped = [[clip(c) for c in row] for row in grid]

    cols = len(clipped[0])
    widths = [0] * cols
    for row in clipped:
        for i, c in enumerate(row):
            widths[i] = max(widths[i], len(c))

    # ì „ì²´ í­ ë„ˆë¬´ ë„“ìœ¼ë©´ í­ ì œí•œ
    total = sum(widths) + (3 * (cols - 1))
    if total > max_width:
        # í­ì´ í° ì»¬ëŸ¼ë¶€í„° ì¡°ê¸ˆì”© ì¤„ì´ê¸°
        order = sorted(range(cols), key=lambda i: widths[i], reverse=True)
        while total > max_width and order:
            i = order[0]
            if widths[i] <= 6:
                order.pop(0)
                continue
            widths[i] -= 1
            total = sum(widths) + (3 * (cols - 1))

    def fmt_row(row):
        out = []
        for i, c in enumerate(row):
            c = clip(c, widths[i])  # ìµœì¢… í­ì— ë§ì¶° í´ë¦½
            out.append(c.ljust(widths[i]))
        return " | ".join(out)

    lines = []
    lines.append(fmt_row(clipped[0]))
    lines.append("-" * min(max_width, len(lines[0])))
    for r in clipped[1:]:
        lines.append(fmt_row(r))

    return "```text\n" + "\n".join(lines) + "\n```"


def fetch_notice_detail(detail_url: str) -> dict:
    res = requests.get(detail_url, timeout=15, headers={"User-Agent": "Mozilla/5.0"})
    res.raise_for_status()
    if not res.encoding:
        res.encoding = res.apparent_encoding

    soup = BeautifulSoup(res.text, "html.parser")

    wrap = soup.select_one(".view_wrap") or soup.body
    if wrap is None:
        return {"text": "", "images": [], "image_blobs": [], "files": []}

    for bad in wrap.select(".view_subject, .meta"):
        bad.decompose()

    for tag in wrap.select("script, style, noscript"):
        tag.decompose()

        # âœ… table ë¨¼ì € ë”°ë¡œ ì¶”ì¶œ (í‘œ ìˆëŠ” ê³µì§€ë§Œ ì²˜ë¦¬)
    table_blocks = []
    for table in wrap.find_all("table"):
        try:
            grid = _table_to_grid(table)
            block = _grid_to_codeblock(grid, max_width=90)
            if block:
                table_blocks.append(block)
        except Exception:
            pass
        table.decompose()  # ë³¸ë¬¸ í…ìŠ¤íŠ¸ì—ì„œ í‘œ ì œê±° (ì¤‘ë³µ/ê¹¨ì§ ë°©ì§€)

    body_text = _extract_body_text(wrap)

    if table_blocks:
        body_text = (body_text + "\n\nğŸ“‹ ì¼ì •í‘œ\n" + "\n".join(table_blocks)).strip()

    # âœ… ì´ë¯¸ì§€ ì²˜ë¦¬: URL ì´ë¯¸ì§€ + data:image(base64) ë¶„ë¦¬
    images: list[str] = []
    image_blobs: list[dict] = []

    for img in wrap.select("img[src]"):
        src = (img.get("src") or "").strip()
        if not src:
            continue

        # 1) ë¡œì»¬ file:// ì€ ì„œë²„/ë´‡ì—ì„œ ê°€ì ¸ì˜¤ê¸° ë¶ˆê°€ â†’ ì œì™¸
        if src.startswith("file://"):
            continue

        # 2) data:image/...;base64,... â†’ ë””ì½”ë”©í•´ì„œ bytesë¡œ ë³´ê´€
        if src.startswith("data:image/"):
            try:
                mime, ext, raw = decode_data_image(src)
                image_blobs.append({"mime": mime, "ext": ext, "bytes": raw})
            except Exception:
                # ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ ìŠ¤í‚µ(í•„ìš”í•˜ë©´ ë¡œê·¸)
                continue
            continue

        # 3) ë‚˜ë¨¸ì§€ â†’ ì ˆëŒ€ URLë¡œ í†µì¼
        images.append(urljoin(detail_url, src))

    # ì¤‘ë³µ ì œê±°(ìˆœì„œ ìœ ì§€)
    images = list(dict.fromkeys(images))

    # âœ… ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬
    files: list[str] = []
    file_box = soup.select_one(".board_file")
    if file_box:
        for a in file_box.select("a[href]"):
            href = (a.get("href") or "").strip()
            if not href:
                continue

            if href.lower().startswith("javascript:"):
                m = DOWN_RE.search(href)
                if m:
                    atch_id, sn = m.group(1), m.group(2)
                    files.append(_build_download_url(detail_url, atch_id, sn))
            else:
                files.append(urljoin(detail_url, href))

        for a in file_box.select("a[onclick]"):
            onclick = a.get("onclick", "")
            m = DOWN_RE.search(onclick)
            if m:
                atch_id, sn = m.group(1), m.group(2)
                files.append(_build_download_url(detail_url, atch_id, sn))

    files = list(dict.fromkeys(files))

    # âœ… watcherì—ì„œ ì“°ëŠ” í‚¤ í¬í•¨í•´ì„œ ë°˜í™˜
    return {
        "text": body_text,
        "images": images,  # URL ì´ë¯¸ì§€
        "image_blobs": image_blobs,  # data:image ë””ì½”ë”© ì´ë¯¸ì§€
        "files": files,
    }
